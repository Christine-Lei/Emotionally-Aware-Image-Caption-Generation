{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfda84c-4caa-4388-a429-e743b6269efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, VisualBertModel,  BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50189daf-2943-441c-9b82-5b3ea81dde7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# from visualbert\n",
    "from processing_image import Preprocess\n",
    "from utils import Config\n",
    "from modeling_frcnn import GeneralizedRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c392e92b-acce-4944-b129-263779440d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    def __init__(self, device='cuda'):\n",
    "        frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "        frcnn_cfg.MODEL.DEVICE = device\n",
    "        self.device = device\n",
    "\n",
    "        self.frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
    "\n",
    "        self.frcnn_cfg = frcnn_cfg\n",
    "        self.image_preprocess = Preprocess(frcnn_cfg)\n",
    "\n",
    "    def get_visual_embeddings(self, image_path):\n",
    "        # run frcnn\n",
    "        images, sizes, scales_yx = self.image_preprocess(image_path)\n",
    "\n",
    "        output_dict = self.frcnn(\n",
    "            images,\n",
    "            sizes,\n",
    "            scales_yx=scales_yx,\n",
    "            padding=\"max_detections\",\n",
    "            max_detections=self.frcnn_cfg.max_detections,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        features = output_dict.get(\"roi_features\").detach().cpu()\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6993ec-50b6-43de-94e0-b5554e475ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, visualbert_model):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.visualbert = visualbert_model\n",
    "        self.fc = nn.Linear(self.visualbert.config.hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, visual_embeds, visual_token_type_ids, visual_attention_mask, labels):\n",
    "        visualbert_outputs = self.visualbert(input_ids=input_ids.squeeze(1),\n",
    "                                             attention_mask=attention_mask.squeeze(1),\n",
    "                                             token_type_ids=token_type_ids.squeeze(1),\n",
    "                                             visual_embeds=visual_embeds.squeeze(1),\n",
    "                                            visual_token_type_ids=visual_token_type_ids.squeeze(1),\n",
    "                                            visual_attention_mask=visual_attention_mask.squeeze(1))\n",
    "        pooled_output = visualbert_outputs['pooler_output']\n",
    "\n",
    "        # Emotion prediction\n",
    "        logits = self.fc(pooled_output) # Loss function operates from logits\n",
    "        \n",
    "        # probabilities = F.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30294568-6ab1-4238-854d-9206bcfcfa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualbert_model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5bb520-1d52-4439-bb18-e81044664667",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in visualbert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961bc2ca-a5c4-42f6-8a43-0845ec0868fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emotions = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5949b54a-3417-4f37-a1b0-eb0bbb2718cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained teacher model\n",
    "teacher_model = TeacherModel(visualbert_model)\n",
    "\n",
    "# Load the state_dict of the model from the .pth file\n",
    "teacher_model_path = 'trained_teacher_model.pth'\n",
    "state_dict = torch.load(teacher_model_path)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "teacher_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41be29ea-6b4a-45ec-844d-2bb25ea8905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMultimodalModel(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim=128):\n",
    "        super(SimpleMultimodalModel, self).__init__()\n",
    "\n",
    "        # Text encoder: Embedding layer + GRU\n",
    "        self.embedding = nn.Embedding(30522, embedding_dim)  # Using 30522 for BERT's Base uncased tokenizer\n",
    "        self.gru = nn.GRU(embedding_dim, 256, batch_first=True)\n",
    "\n",
    "        # Image encoder: Simplified CNN\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fusion layer and classifier\n",
    "        self.fc1 = nn.Linear(64 + 256, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # Process text input through embedding and GRU\n",
    "        x_text = self.embedding(input_ids)\n",
    "        _, x_text = self.gru(x_text)\n",
    "        x_text = x_text.squeeze(0)  # Remove sequence dimension\n",
    "\n",
    "        # Process images through simplified CNN\n",
    "        x_img = F.relu(self.conv1(images))\n",
    "        x_img = F.relu(self.conv2(x_img))\n",
    "        x_img = F.relu(self.conv3(x_img))\n",
    "        x_img = self.adaptive_pool(x_img)\n",
    "        x_img = torch.flatten(x_img, 1)  # Flatten all dimensions except batch\n",
    "\n",
    "        # Fusion and classification\n",
    "        combined_features = torch.cat((x_img, x_text), dim=1)\n",
    "        fused_features = F.relu(self.fc1(combined_features))\n",
    "        logits = self.fc2(fused_features)\n",
    "\n",
    "        # Apply sigmoid to convert logits to probabilities\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        \n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8442f159-7f2a-425b-9620-3fdd6cd7d3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained teacher model\n",
    "student_model = SimpleMultimodalModel(num_emotions)\n",
    "\n",
    "# Load the state_dict of the model from the .pth file\n",
    "student_model_path = 'trained_student_model.pth'\n",
    "state_dict = torch.load(student_model_path)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "student_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8750fa-6650-4670-9067-961331dc6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pickle\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaafa274-0210-4c94-a672-3cd4da38b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coco_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d632757a-f4c7-469a-8a7e-a34e5fb7324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetA --> COCO\n",
    "class DatasetA(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        self.tokenizer = tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_url = row['coco_url']\n",
    "        caption = eval(row['captions'])[0]  # Evaluating the string to get the list and taking the first item\n",
    "\n",
    "        # Image processing\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')  # Convert image to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Text processing\n",
    "        inputs = self.tokenizer(caption, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "        input_ids, attention_mask = inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Labels - extracting the last 29 columns as classes\n",
    "        labels = torch.tensor(row[2:].values.astype(float), dtype=torch.float32)\n",
    "\n",
    "        return input_ids, attention_mask, image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f061773-0685-4a91-ac55-7e0191c5058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(url):\n",
    "    # Load image from URL\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "def prepare_text(text):\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    return inputs['input_ids'], inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48c37321-887a-4186-b450-83878ba8cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_A = DatasetA(df.sample(frac=0.1, random_state=42) )\n",
    "dataloader_A = DataLoader(dataset_A, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff9e4a6e-1a90-439b-8d1d-5b6da35bf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dee84bb-bc4e-4a31-ae92-3f7ff900036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetB --> Cleaned Socratis\n",
    "class DatasetB(Dataset):\n",
    "    def __init__(self, data, images_base_path, device='cuda'):\n",
    "        self.df = data\n",
    "        self.images_base_path = images_base_path\n",
    "\n",
    "        # feature extractors\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "        self.visual_extractor = ImageProcessor(device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        captions, image_name = self.df.iloc[idx]['caption'], self.df.iloc[idx]['image_name']\n",
    "\n",
    "        # get image embedings\n",
    "        image_path = os.path.join(self.images_base_path, image_name)\n",
    "        visual_embeds = self.visual_extractor.get_visual_embeddings(image_path)\n",
    "        visual_token_type_ids = torch.ones(\n",
    "            visual_embeds.shape[:-1], dtype=torch.long)\n",
    "        visual_attention_mask = torch.ones(\n",
    "            visual_embeds.shape[:-1], dtype=torch.float)\n",
    "\n",
    "        # get text embeddings\n",
    "        inputs = self.tokenizer(captions, return_tensors=\"pt\", max_length=32, truncation=True, padding='max_length')\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        emotions_string = self.df['emotions'][idx]\n",
    "        emotions_list = eval(emotions_string)\n",
    "        one_hot_encoded = torch.zeros(len(label_map))\n",
    "        for emotion in emotions_list:\n",
    "            if emotion in label_map:\n",
    "                idx = label_map[emotion]\n",
    "                one_hot_encoded[idx] = 1\n",
    "\n",
    "        labels = one_hot_encoded\n",
    "\n",
    "        return (input_ids, token_type_ids, attention_mask,\n",
    "                visual_embeds, visual_token_type_ids, visual_attention_mask,\n",
    "                labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92521de3-311d-4d7b-8af9-0a3daa8067d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'curious': 0,\n",
    " 'amazed': 1,\n",
    " 'fear': 2,\n",
    " 'awe': 3,\n",
    " 'neutral': 4,\n",
    " 'disgusted': 5,\n",
    " 'worried': 6,\n",
    " 'intrigued': 7,\n",
    " 'confused': 8,\n",
    " 'beautiful': 9,\n",
    " 'happy': 10,\n",
    " 'annoyed': 11,\n",
    " 'impressed': 12,\n",
    " 'sad': 13,\n",
    " 'proud': 14,\n",
    " 'inspired': 15,\n",
    " 'angry': 16,\n",
    " 'excited': 17,\n",
    " 'nostalgic': 18,\n",
    " 'upset': 19,\n",
    " 'concerned': 20,\n",
    " 'good': 21,\n",
    " 'hopeful': 22,\n",
    " 'anger': 23,\n",
    " 'joy': 24,\n",
    " 'interested': 25,\n",
    " 'calm': 26,\n",
    " 'bored': 27,\n",
    " 'scared': 28}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "495b3b37-600a-4f08-b6cb-171abfc2634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /home/ota231/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "images_path = './images'\n",
    "dataset_B = DatasetB(df, images_path)\n",
    "dataloader_B = DataLoader(dataset_B, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf194e35-4dc2-4fc7-b26e-a5a4f1128321",
   "metadata": {},
   "source": [
    "## Transfer: AnB+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cef2a218-f72b-478e-b905-ec8e9aadfdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained weights for both student (model A) and teacher (model B)\n",
    "model_A = SimpleMultimodalModel(num_classes=num_emotions)\n",
    "model_A.load_state_dict(torch.load(student_model_path))\n",
    "\n",
    "model_B = TeacherModel(visualbert_model)\n",
    "model_B.load_state_dict(torch.load(teacher_model_path))\n",
    "\n",
    "modified_fc1_weights = model_A.fc1.weight.data.T.mm(model_A.fc2.weight.data.T)\n",
    "modified_fc1_bias = model_A.fc2.bias.data.unsqueeze(0)\n",
    "\n",
    "transformed_weights = model_B.fc.weight.data.T.mm(modified_fc1_weights.T).mm(modified_fc1_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfc5fc21-64be-4921-b613-b1f6160f85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_B.fc.weight.data.copy_(transformed_weights.T)\n",
    "    model_B.fc.bias.data.copy_(modified_fc1_bias.data.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60481623-cde5-4e9d-8321-f0cc5589fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88854bb2-a3d6-4bb6-8468-f9a792403c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_B.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0de9da8-cee9-45b1-bf41-b7c32137c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc5d9873-f0f2-4ebe-ba82-29d00e6381a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|                                 | 0/185 [00:13<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TeacherModel.forward() missing 4 required positional arguments: 'visual_embeds', 'visual_token_type_ids', 'visual_attention_mask', and 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m model_B(input_ids,  attention_mask, images)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(probabilities, labels)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: TeacherModel.forward() missing 4 required positional arguments: 'visual_embeds', 'visual_token_type_ids', 'visual_attention_mask', and 'labels'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model_B.train()\n",
    "    running_loss = 0.\n",
    "    \n",
    "    # Wrap the dataloader_A with tqdm for progress bar\n",
    "    with tqdm(total=len(dataloader_A), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for batch in dataloader_A:\n",
    "            input_ids, attention_mask, images, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            probabilities = model_B(input_ids,  attention_mask, images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(probabilities, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            pbar.update(1)  # Update tqdm progress bar\n",
    "\n",
    "    # Calculate average training loss\n",
    "    epoch_loss = running_loss / len(dataloader_A.dataset)\n",
    "    \n",
    "    print(f'Train Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fd7a0-0d33-48e7-acb4-2645d5e3d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_B.state_dict(), 'transfer_anb_plus.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a81c2b-a663-4332-8978-0e98a4d042ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Custom Env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
