{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e818dc66-4d8d-43ac-bdd0-74aff8f2016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, VisualBertModel,  BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c5c809-2773-47f0-a566-77c46179a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# from visualbert\n",
    "from processing_image import Preprocess\n",
    "from utils import Config\n",
    "from modeling_frcnn import GeneralizedRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e406b8-866e-45ce-ab38-463cbfdfabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    def __init__(self, device='cuda'):\n",
    "        frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "        frcnn_cfg.MODEL.DEVICE = device\n",
    "        self.device = device\n",
    "\n",
    "        self.frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
    "\n",
    "        self.frcnn_cfg = frcnn_cfg\n",
    "        self.image_preprocess = Preprocess(frcnn_cfg)\n",
    "\n",
    "    def get_visual_embeddings(self, image_path):\n",
    "        # run frcnn\n",
    "        images, sizes, scales_yx = self.image_preprocess(image_path)\n",
    "\n",
    "        output_dict = self.frcnn(\n",
    "            images,\n",
    "            sizes,\n",
    "            scales_yx=scales_yx,\n",
    "            padding=\"max_detections\",\n",
    "            max_detections=self.frcnn_cfg.max_detections,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        features = output_dict.get(\"roi_features\").detach().cpu()\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d9795-e069-4cc9-88de-677bda9337ab",
   "metadata": {},
   "source": [
    "## View Configuration of Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc647517-3c5d-4e2a-ab8d-d3bed18beff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, visualbert_model):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.visualbert = visualbert_model\n",
    "        self.fc = nn.Linear(self.visualbert.config.hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, visual_embeds, visual_token_type_ids, visual_attention_mask, labels):\n",
    "        visualbert_outputs = self.visualbert(input_ids=input_ids.squeeze(1),\n",
    "                                             attention_mask=attention_mask.squeeze(1),\n",
    "                                             token_type_ids=token_type_ids.squeeze(1),\n",
    "                                             visual_embeds=visual_embeds.squeeze(1),\n",
    "                                            visual_token_type_ids=visual_token_type_ids.squeeze(1),\n",
    "                                            visual_attention_mask=visual_attention_mask.squeeze(1))\n",
    "        pooled_output = visualbert_outputs['pooler_output']\n",
    "\n",
    "        # Emotion prediction\n",
    "        logits = self.fc(pooled_output) # Loss function operates from logits\n",
    "        \n",
    "        # probabilities = F.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099ab693-f365-4163-a740-614d13fa49ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualbert_model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657a31ff-cd0d-4b0a-8ddd-a6fd0da344f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in visualbert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26e0b8fc-44e0-4f54-abe6-a9a77af661b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emotions = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6e6c53-1985-4668-a274-0aafbad7d54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TeacherModel(\n",
      "  (visualbert): VisualBertModel(\n",
      "    (embeddings): VisualBertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (visual_token_type_embeddings): Embedding(2, 768)\n",
      "      (visual_position_embeddings): Embedding(512, 768)\n",
      "      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "    )\n",
      "    (encoder): VisualBertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x VisualBertLayer(\n",
      "          (attention): VisualBertAttention(\n",
      "            (self): VisualBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): VisualBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): VisualBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): VisualBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): VisualBertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=768, out_features=29, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the trained teacher model\n",
    "teacher_model = TeacherModel(visualbert_model)\n",
    "\n",
    "# Load the state_dict of the model from the .pth file\n",
    "teacher_model_path = 'trained_teacher_model.pth'\n",
    "state_dict = torch.load(teacher_model_path)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "teacher_model.load_state_dict(state_dict)\n",
    "\n",
    "# Print the model architecture\n",
    "print(teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f1a77-b3e0-49fa-9839-ccf8e44bcc79",
   "metadata": {},
   "source": [
    "## View Configuration of Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fb40d3-4961-4f76-b6a3-59ab17cff3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMultimodalModel(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim=128):\n",
    "        super(SimpleMultimodalModel, self).__init__()\n",
    "\n",
    "        # Text encoder: Embedding layer + GRU\n",
    "        self.embedding = nn.Embedding(30522, embedding_dim)  # Using 30522 for BERT's Base uncased tokenizer\n",
    "        self.gru = nn.GRU(embedding_dim, 256, batch_first=True)\n",
    "\n",
    "        # Image encoder: Simplified CNN\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fusion layer and classifier\n",
    "        self.fc1 = nn.Linear(64 + 256, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # Process text input through embedding and GRU\n",
    "        x_text = self.embedding(input_ids)\n",
    "        _, x_text = self.gru(x_text)\n",
    "        x_text = x_text.squeeze(0)  # Remove sequence dimension\n",
    "\n",
    "        # Process images through simplified CNN\n",
    "        x_img = F.relu(self.conv1(images))\n",
    "        x_img = F.relu(self.conv2(x_img))\n",
    "        x_img = F.relu(self.conv3(x_img))\n",
    "        x_img = self.adaptive_pool(x_img)\n",
    "        x_img = torch.flatten(x_img, 1)  # Flatten all dimensions except batch\n",
    "\n",
    "        # Fusion and classification\n",
    "        combined_features = torch.cat((x_img, x_text), dim=1)\n",
    "        fused_features = F.relu(self.fc1(combined_features))\n",
    "        logits = self.fc2(fused_features)\n",
    "\n",
    "        # Apply sigmoid to convert logits to probabilities\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        \n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58813534-e574-4628-a828-857c9a2d277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMultimodalModel(\n",
      "  (embedding): Embedding(30522, 128)\n",
      "  (gru): GRU(128, 256, batch_first=True)\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (adaptive_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=29, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the trained teacher model\n",
    "student_model = SimpleMultimodalModel(num_emotions)\n",
    "\n",
    "# Load the state_dict of the model from the .pth file\n",
    "student_model_path = 'trained_student_model.pth'\n",
    "state_dict = torch.load(student_model_path)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "student_model.load_state_dict(state_dict)\n",
    "\n",
    "# Print the model architecture\n",
    "print(student_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c5d74-a3e5-4f4c-93d4-06695a2f49d8",
   "metadata": {},
   "source": [
    "# Transfer Learning Overview\n",
    "Given our source dataset, COCO (A), and target dataset, Socratis (B), we will experiment with 2 finetuning approaches based on [Yosinski et al](https://proceedings.neurips.cc/paper_files/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf). In both cases, initialize the parameters of the teacher model (B) with the pre-trained weights of the student model (A).\n",
    "- AnB: finetuning but keeping all the layers frozen\n",
    "- AnB+: finetuning but not freezing the layers\n",
    "\n",
    "However, we can see that the architectures and dimensions for models A and B are very different so here is how we account for that:\n",
    "we initialize the last layer (fc) in model B with a modified version of the last two layers (fc1 and fc2) in model A. This involves adapting the dimensions of the weights to ensure compatibility between the models. In the AnB case, we would freeze all layers in model B except the last layer (fc), allowing it to adapt to the target task while keeping the rest of the model fixed. Conversely, in the AnB+ case, we would not freeze any layer in model B, enabling all layers, including the last layer (fc), to be fine-tuned on the target dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95af0499-cf32-4602-b946-5bea3b03dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pickle\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71047c3d-5c75-4a46-9f89-bee662e3f324",
   "metadata": {},
   "source": [
    "## Load Datasets A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89e269-afe7-44e7-9207-0a206b31b3ca",
   "metadata": {},
   "source": [
    "### Dataset A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7520c35-e377-4ffd-84fc-cd7412da35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coco_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8176ed43-6658-4828-bab0-2ed6b992be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetA --> COCO\n",
    "class DatasetA(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        self.tokenizer = tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_url = row['coco_url']\n",
    "        caption = eval(row['captions'])[0]  # Evaluating the string to get the list and taking the first item\n",
    "\n",
    "        # Image processing\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')  # Convert image to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Text processing\n",
    "        inputs = self.tokenizer(caption, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "        input_ids, attention_mask = inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Labels - extracting the last 29 columns as classes\n",
    "        labels = torch.tensor(row[2:].values.astype(float), dtype=torch.float32)\n",
    "\n",
    "        return input_ids, attention_mask, image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbd6bc76-c1b0-4dc2-b1ff-3c0c326acf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(url):\n",
    "    # Load image from URL\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "def prepare_text(text):\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    return inputs['input_ids'], inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "355cac9b-0315-445c-8d85-c21cc3435930",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_A = DatasetA(df.sample(frac=0.1, random_state=42) )\n",
    "dataloader_A = DataLoader(dataset_A, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "671fc671-b9bb-4b1b-aabf-85dcc83001f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  1037, 18097, 13184,  2099,  2279,  2000,  1037, 16247,  2327,\n",
       "         17428,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([[[0.2314, 0.2235, 0.2314,  ..., 0.3020, 0.2980, 0.2902],\n",
       "          [0.2392, 0.2392, 0.2392,  ..., 0.3020, 0.2941, 0.2824],\n",
       "          [0.2392, 0.2431, 0.2471,  ..., 0.3020, 0.2863, 0.2784],\n",
       "          ...,\n",
       "          [0.0039, 0.0000, 0.0000,  ..., 0.1608, 0.1569, 0.1529],\n",
       "          [0.0039, 0.0000, 0.0000,  ..., 0.1569, 0.1529, 0.1451],\n",
       "          [0.0000, 0.0039, 0.0039,  ..., 0.1529, 0.1490, 0.1373]],\n",
       " \n",
       "         [[0.2078, 0.2078, 0.2157,  ..., 0.2510, 0.2431, 0.2353],\n",
       "          [0.2118, 0.2118, 0.2118,  ..., 0.2510, 0.2471, 0.2471],\n",
       "          [0.2118, 0.2118, 0.2118,  ..., 0.2510, 0.2510, 0.2549],\n",
       "          ...,\n",
       "          [0.0039, 0.0000, 0.0000,  ..., 0.1647, 0.1608, 0.1569],\n",
       "          [0.0039, 0.0000, 0.0000,  ..., 0.1608, 0.1569, 0.1529],\n",
       "          [0.0000, 0.0039, 0.0039,  ..., 0.1569, 0.1529, 0.1451]],\n",
       " \n",
       "         [[0.1529, 0.1490, 0.1490,  ..., 0.1608, 0.1569, 0.1686],\n",
       "          [0.1529, 0.1490, 0.1529,  ..., 0.1725, 0.1686, 0.1725],\n",
       "          [0.1490, 0.1569, 0.1608,  ..., 0.1686, 0.1647, 0.1725],\n",
       "          ...,\n",
       "          [0.0039, 0.0000, 0.0000,  ..., 0.1451, 0.1373, 0.1373],\n",
       "          [0.0039, 0.0000, 0.0000,  ..., 0.1412, 0.1333, 0.1294],\n",
       "          [0.0000, 0.0039, 0.0039,  ..., 0.1373, 0.1294, 0.1255]]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_A[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce4e0f2-d454-451d-8ddc-f9c6ab8a7e0c",
   "metadata": {},
   "source": [
    "### Dataset B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67ac1ea9-8eeb-439b-9ded-f1b92ccc0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b605272-4ab1-417d-a391-ce4ace9dd627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetB --> Cleaned Socratis\n",
    "class DatasetB(Dataset):\n",
    "    def __init__(self, data, images_base_path, device='cuda'):\n",
    "        self.df = data\n",
    "        self.images_base_path = images_base_path\n",
    "\n",
    "        # feature extractors\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "        self.visual_extractor = ImageProcessor(device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        captions, image_name = self.df.iloc[idx]['caption'], self.df.iloc[idx]['image_name']\n",
    "\n",
    "        # get image embedings\n",
    "        image_path = os.path.join(self.images_base_path, image_name)\n",
    "        visual_embeds = self.visual_extractor.get_visual_embeddings(image_path)\n",
    "        visual_token_type_ids = torch.ones(\n",
    "            visual_embeds.shape[:-1], dtype=torch.long)\n",
    "        visual_attention_mask = torch.ones(\n",
    "            visual_embeds.shape[:-1], dtype=torch.float)\n",
    "\n",
    "        # get text embeddings\n",
    "        inputs = self.tokenizer(captions, return_tensors=\"pt\", max_length=32, truncation=True, padding='max_length')\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        emotions_string = self.df['emotions'][idx]\n",
    "        emotions_list = eval(emotions_string)\n",
    "        one_hot_encoded = torch.zeros(len(label_map))\n",
    "        for emotion in emotions_list:\n",
    "            if emotion in label_map:\n",
    "                idx = label_map[emotion]\n",
    "                one_hot_encoded[idx] = 1\n",
    "\n",
    "        labels = one_hot_encoded\n",
    "\n",
    "        return (input_ids, token_type_ids, attention_mask,\n",
    "                visual_embeds, visual_token_type_ids, visual_attention_mask,\n",
    "                labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d38b59e-417f-4854-84b8-32c497553ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'curious': 0,\n",
    " 'amazed': 1,\n",
    " 'fear': 2,\n",
    " 'awe': 3,\n",
    " 'neutral': 4,\n",
    " 'disgusted': 5,\n",
    " 'worried': 6,\n",
    " 'intrigued': 7,\n",
    " 'confused': 8,\n",
    " 'beautiful': 9,\n",
    " 'happy': 10,\n",
    " 'annoyed': 11,\n",
    " 'impressed': 12,\n",
    " 'sad': 13,\n",
    " 'proud': 14,\n",
    " 'inspired': 15,\n",
    " 'angry': 16,\n",
    " 'excited': 17,\n",
    " 'nostalgic': 18,\n",
    " 'upset': 19,\n",
    " 'concerned': 20,\n",
    " 'good': 21,\n",
    " 'hopeful': 22,\n",
    " 'anger': 23,\n",
    " 'joy': 24,\n",
    " 'interested': 25,\n",
    " 'calm': 26,\n",
    " 'bored': 27,\n",
    " 'scared': 28}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "167a3513-84f4-4417-b8e2-53edbb6e0661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /home/ota231/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "images_path = './images'\n",
    "dataset_B = DatasetB(df, images_path)\n",
    "dataloader_B = DataLoader(dataset_B, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17463d23-cf7a-4067-a7c1-3de577285760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 2711, 2006, 2143, 2839, 2096, 3403, 2005, 3185, 2000, 2272, 2041,\n",
       "           102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([[[6.1835e-02, 0.0000e+00, 3.7392e-01,  ..., 0.0000e+00,\n",
       "           2.6447e-01, 1.6728e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 1.1359e-01,  ..., 1.3758e-02,\n",
       "           0.0000e+00, 2.9012e-01],\n",
       "          [0.0000e+00, 0.0000e+00, 1.4732e-01,  ..., 0.0000e+00,\n",
       "           8.1740e-04, 2.3377e+00],\n",
       "          ...,\n",
       "          [0.0000e+00, 5.5369e-02, 7.3985e-02,  ..., 2.1606e-01,\n",
       "           2.9640e-01, 1.2079e-02],\n",
       "          [0.0000e+00, 0.0000e+00, 1.1523e-01,  ..., 0.0000e+00,\n",
       "           2.5727e+00, 8.4765e-01],\n",
       "          [3.3876e-01, 3.5944e-01, 3.5183e-01,  ..., 0.0000e+00,\n",
       "           3.5316e-02, 8.4532e-01]]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_B[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bbfaf3-14dc-417e-b4ff-fa3dac78111b",
   "metadata": {},
   "source": [
    "## Transfer Learning: Account for Architectural Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd5e377d-ff3b-4a4d-9971-e25a1c2f551e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained weights for both student (model A) and teacher (model B)\n",
    "model_A = SimpleMultimodalModel(num_classes=num_emotions)\n",
    "model_A.load_state_dict(torch.load(student_model_path))\n",
    "\n",
    "model_B = TeacherModel(visualbert_model)\n",
    "model_B.load_state_dict(torch.load(teacher_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71ba4b94-49ff-4a66-870d-ce53f5992d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 320])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A.fc1.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4250fa69-d900-474b-a773-19af5021825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A.fc2.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75f57b68-3a84-4297-8f1f-4a8183704e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose first\n",
    "modified_fc1_weights = model_A.fc1.weight.data.T.mm(model_A.fc2.weight.data.T)\n",
    "modified_fc1_bias = model_A.fc2.bias.data.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac1044fb-5830-460e-83e6-9c33c6d0ea84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([320, 29]), torch.Size([1, 29]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_fc1_weights.shape, modified_fc1_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0439ed5c-cab1-4edb-8c17-960a7c969284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([29, 768]), torch.Size([29]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.fc.weight.data.shape, model_B.fc.bias.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84a7d9c3-e8f6-4dac-a10e-d7e7a89d217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear transformation to match dimensions of fc in model B\n",
    "transformed_weights = model_B.fc.weight.data.T.mm(modified_fc1_weights.T).mm(modified_fc1_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a26cd9a-59e7-4453-b340-a2cf2fee8bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 29]), torch.Size([1, 29]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_weights.shape, modified_fc1_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a338086-02f9-482b-9f04-45e58aa0f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_B.fc.weight.data.copy_(transformed_weights.T)\n",
    "    model_B.fc.bias.data.copy_(modified_fc1_bias.data.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22ea74e4-cca0-438c-bd0b-986e9df4133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa6f13-ce31-41a7-9498-f3d1db052099",
   "metadata": {},
   "source": [
    "## Transfer Learning AnB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2057cf20-8eeb-46c9-b70d-29ba670b28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pre-trained layers except the final linear layer of the teacher model (B)\n",
    "for param in model_B.parameters():\n",
    "    param.requires_grad = False\n",
    "model_B.fc.weight.requires_grad = True\n",
    "model_B.fc.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95a00d27-7198-4e23-ab77-30aba0bc9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_B.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b50cb95-02b9-4b04-b65b-df43c49fe3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3ce1b1a-60d6-47f6-ab35-b655fe54cff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|                                 | 0/185 [00:10<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TeacherModel.forward() missing 4 required positional arguments: 'visual_embeds', 'visual_token_type_ids', 'visual_attention_mask', and 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m model_B(input_ids,  attention_mask, images)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(probabilities, labels)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: TeacherModel.forward() missing 4 required positional arguments: 'visual_embeds', 'visual_token_type_ids', 'visual_attention_mask', and 'labels'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model_B.train()\n",
    "    running_loss = 0.\n",
    "    \n",
    "    # Wrap the dataloader_A with tqdm for progress bar\n",
    "    with tqdm(total=len(dataloader_A), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for batch in dataloader_A:\n",
    "            input_ids, attention_mask, images, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            probabilities = model_B(input_ids,  attention_mask, images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(probabilities, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            pbar.update(1)  # Update tqdm progress bar\n",
    "\n",
    "    # Calculate average training loss\n",
    "    epoch_loss = running_loss / len(dataloader_A.dataset)\n",
    "    \n",
    "    print(f'Train Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f25a5-3a9a-4aad-b61e-3676ef567f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_B.state_dict(), 'transfer_anb.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Custom Env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
